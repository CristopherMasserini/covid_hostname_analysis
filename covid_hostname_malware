"""
Author: Cristopher Masserini

Can be found at: https://github.com/CristopherMasserini/covid_hostname_analysis

A project that takes a file of hostnames that have a reference to COVID-19 and attempts to separate the malicious
from the valid. The goal here was to get a viable package available as quickly as possible to help mitigate the affect
of opportunistic attackers.

Due to what is being used from these packages, the newest versions of these packages and of Python is required
"""

# ===Imports===
import requests
from datetime import datetime
import pandas as pd
import whois
import subprocess


# ===Processing===

# This takes the date and time, combines them and converts it into seconds since the Epoch for easy calculation
# Also puts the hostnames into its list
# date_to_start is the date where you want to start the processing. Useful this is being more than once. This way,
# You do not need to process all the data each time you want to update your data
def processing(text_list: str, date_to_start: str) -> pd.DataFrame:
    dates_of_record = list()  # The date and time of the hostname creation using date time
    hostnames = list()  # The hostname
    # Ignores the description before the data starts
    start_index = text_list.index("date,time,hostname")
    for record in text_list[start_index + 1:]:
        try:
            # Splits the record into date, time, and hostname
            record_list = record.split(',')
            record_datetime_str = record_list[0] + " " + record_list[1]  # Concatenates the date and time
            hostname = record_list[2]  # The host name
            record_datetime = datetime.strptime(record_datetime_str, '%Y-%m-%d %H:%M:%S.%f')  # Converts to datetime
            start_date = datetime.strptime(date_to_start, '%Y-%m-%d %H:%M:%S.%f')  # Converts start date to datetime

            if record_datetime >= start_date:
                seconds_of_record.append(record_datetime.timestamp())
                dates_of_record.append(record_datetime)  # Puts the creation time into a list
                hostnames.append(hostname)  # Puts the hostname into a list
        except IndexError:  # Got to the last record in the list
            pass

    # Puts the seconds since Epoch and hostname into a data frame. This is used so it can be sorted by the seconds.
    # This is because at the time of writing this code, not all the data is sorted according to date and time correctly.
    pd.options.display.float_format = '{:10f}'.format
    data = {"Hostname": hostnames, "Hostname Creation Date": dates_of_record}
    df = pd.DataFrame(data)
    df = df.sort_values(by=["Hostname Creation Date"])
    df.index = range(len(df))  # Resets the indexing after the the sorting
    return df


# ===Scoring===

# Create a score. So depending on the time diff column and the amount of time since the base domain was created,
# the hostname will get a score. Smaller the score, the higher the risk of being malicious.
# For time difference: less than one second gets a score of 1, between one and 5 seconds gets a score of 5,
# Between 5 and 10 seconds gets a score of 10, above 10 seconds gets a score of 15
# I give an intermediate score of 7 for time that equal zero. This is because of the issue that arises outlined
# In the file of the first hostnames with the same time for date and time. Since it is not known, time cannot be
# An overwhelming factor in score, right now. Can be fixed in the future
def creation_scoring(df: pd.DataFrame) -> pd.DataFrame:
    # This calculates the difference in time that a host name was created since the last host name creation time.
    time_since_last = list()  # Used to see time between hostname creations
    score = list()  # Used to score the record
    for i in range(0, len(seconds_of_record)):
        print(i)
        time = 0.0
        if i == 0:
            time_since_last.append(time)
        else:
            try:
                time = seconds_of_record[i] - seconds_of_record[i - 1]
                time_since_last.append(time)
            except IndexError:
                time = seconds_of_record[i] - seconds_of_record[i - 1]
                time_since_last.append(time)

        # The actual scoring
        if time == 0.0:
            score.append(7)
        elif 0 < time <= 1:
            score.append(1)
        elif 1 < time <= 5:
            score.append(5)
        elif 5 < time <= 10:
            score.append(10)
        else:
            score.append(15)

    # Adds the difference in time and score to the data frame

    df.loc[:, "Time diff"] = time_since_last
    df.loc[:, "Time Score"] = score

    # Saving an intermediate file
    write_csv(df, r'HostnameData_Intermediate1.csv', "write")
    return df


# Gives a score for how ling the base domain has been active. For instance, any hostname with duke.edu base domain
# Will come up with a creation date time of 1986-06-02 00:00:00.
# Less than one year, score is 1, between one year and two years gets a score of 5
# Between two year and five years gets a score of 10, and greater than 5 years gets a score of 10
# Because whois library does not recognize some TLD, like .pt which is the portugal country code TLD,
# Any hostnames with this problem get an intermediate score of 7
# If a creation date is not listed or wrong format, given an intermediate score of 7
# Other errors are given a score of 1
def base_domain_scoring(df: pd.DataFrame) -> pd.DataFrame:
    # Assuming a 365 day year and no leap seconds, a good enough calculation
    score_new = list()
    domain_creation_date = list()

    # Amount of time to be used in the scoring
    now = datetime.utcnow().date()

    days_one_year = 365
    days_two_years = 2 * days_one_year
    days_five_years = 5 * days_one_year

    with open("exceptions_file.txt", "w") as file:
        file.write("exception,hostname\n")

        names = list(df.loc[:, "Hostname"])
        hostname_creation_dates = list(df.loc[:, "Hostname Creation Date"])
        for i in range(0, len(names)):
            print("Part 2: {}".format(i))
            name = names[i]
            try:

                # A lot of the hostnames on the list are already base domains, so do not need to look those up
                # Just a way to save time, otherwise it has a really long run time
                if len(name.split('.')) > 2:
                    hostname_creation_date = whois.query(name).creation_date.date()
                else:
                    hostname_creation_date = hostname_creation_dates[i].date()

                domain_creation_date.append(hostname_creation_date)

                diff = now - hostname_creation_date
                diff_days = diff.days

                # The scoring for time since the base domain was created.
                if diff_days < days_one_year:
                    score_new.append(1)
                elif days_one_year <= diff_days < days_two_years:
                    score_new.append(5)
                elif days_two_years <= diff_days < days_five_years:
                    score_new.append(10)
                elif days_five_years <= diff_days:
                    score_new.append(15)
            except AttributeError:
                # Some of the hostnames don't give a creation date
                file.write("AttributeError,{}\n".format(name))
                score_new.append(7)
                domain_creation_date.append(float('NaN'))
            except UnicodeDecodeError:
                # Some of the hostnames have non-unicode names
                file.write('UnicodeDecodeError,{}\n'.format(name))
                score_new.append(1)
                domain_creation_date.append(float('NaN'))
            except KeyError:
                file.write('Key Error,{}\n'.format(name))
                score_new.append(1)
                domain_creation_date.append(float('NaN'))
            except whois.exceptions.WhoisCommandFailed:
                # Some hostnames show this for unknown reasons
                file.write('whois.exceptions.WhoisCommandFailed,{}\n'.format(name))
                score_new.append(1)
                domain_creation_date.append(float('NaN'))
            except whois.exceptions.FailedParsingWhoisOutput:
                # When not running from an IDE, some hostnames show this error
                file.write('whois.exceptions.FailedParsingWhoisOutput,{}\n'.format(name))
                score_new.append(1)
                domain_creation_date.append(float('NaN'))
            except whois.exceptions.UnknownDateFormat:
                # Some of the hostnames don't give a creation date in correct format
                file.write('whois.exceptions.UnknownDateFormat,{}\n'.format(name))
                score_new.append(7)
                domain_creation_date.append(float('NaN'))
            except whois.exceptions.UnknownTld:
                # This is done for the problem described above with TLD
                file.write('whois.exceptions.UnknownTld,{}\n'.format(name))
                score_new.append(7)
                domain_creation_date.append(float('NaN'))

    df.loc[:, "Base Domain Creation Date"] = pd.Series(domain_creation_date, index=df.index)
    df.loc[:, "Base Domain Score"] = pd.Series(score_new, index=df.index)

    # Saving an intermediate file
    write_csv(df, r'HostnameData_Intermediate2.csv', "write")

    return df


# Add the IP Address of the hostname to the data frame.
# This can be used to see which hostnames all have the same IP Address
# Many hostnames all with the same IP can be a sign of a phishy domain
def ip_inclusion(df: pd.DataFrame) -> pd.DataFrame:
    # For IP Address lookup
    global ip_distinct
    global ip_count

    ip_list = list()
    names = list(df.loc[:, "Hostname"])
    for i in range(0, len(names)):
        print("Part 3: {}".format(i))
        # Looks up the IP Address using nslookup
        cmd = 'nslookup {} | grep Address: |grep -v -e "#"'.format(names[i])
        ps = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        output = str(ps.communicate()[0])
        ip = output.split(r'\n')[0][11:]
        ip_list.append(ip)

        # Adds the IP to the data frame
        if ip not in ip_distinct:
            ip_distinct.append(ip)
            ip_count.append(1)
        else:
            _ = ip_distinct.index(ip)
            count = ip_count[_] + 1
            ip_count[_] = count

    df.loc[:, "IP Addresses"] = pd.Series(ip_list, index=df.index)

    # Saving an intermediate file
    write_csv(df, r'HostnameData_Intermediate3.csv', "write")

    return df


# Calculates a final score based off of the base domain score, the creation time score,
# and how many times the same IP address came up in the file.
# This also puts the scoring in line with industry standard.
def final_score(df: pd.DataFrame) -> pd.DataFrame:
    global ip_distinct
    global ip_count
    end_score = list()
    time_score = list(df.loc[:, "Time Score"])
    domain_score = list(df.loc[:, "Base Domain Score"])
    ip = list(df.loc[:, "IP Addresses"])

    # If IP only comes up one time, deemed the safest
    # 2 or 3 times is acceptable but sometimes worrisome
    # More than 3 times, this is highest risk with the exception of no IP given, which is given the lowest risk rating
    for i in range(0, len(time_score)):
        score_1 = time_score[i] + domain_score[i]
        print("IP Score: {}".format(i))

        # The end scores are made to be out of 100, inline with industry standards
        _ = ip_count[ip_distinct.index(ip[i])]
        if _ == 1:
            end_score.append(100 - (score_1 + 10))
        elif 2 <= _ <= 3:
            end_score.append(100 - (score_1 + 5))
        else:
            if ip == "":
                end_score.append(100 - (score_1 + 10))
            else:
                end_score.append(100 - (score_1 + 1))

    df.loc[:, "Final Score"] = pd.Series(end_score, index=df.index)
    return df


# ===Writing to a CSV===

# Writes to a csv. Either writing a new file or appending
# df is data to be added
# name is name of file
# selection is if it is to be appended or a new file
def write_csv(df: pd.DataFrame, name: str, selection: str):
    if selection == "append":
        df.to_csv(name, mode='a', header=False, index=False)
    else:
        df.to_csv(name, index=False)


if __name__ == '__main__':
    # Getting the data from the link
    # This then splits the file into individual strings at new line characters
    link = 'https://1984.sh/covid19-domains-feed.txt'
    f = requests.get(link)
    link_text = f.text.split('\n')

    # List for timestamps
    seconds_of_record = list()

    # The date and time to start at
    # 2020-03-14 15:11:20.684411 is the very beginning of the file
    initial_date = "2020-03-14 15:11:20.684411"

    # Lists for the IPs needed
    ip_distinct = list()
    ip_count = list()

    # Calling the functions to create the data frame
    initial_df = creation_scoring(processing(link_text, initial_date))
    df_base_domain = base_domain_scoring(initial_df)
    df_with_ips = ip_inclusion(df_base_domain)
    final_df = final_score(df_with_ips)

    # Creating the CSV
    write_csv(final_df, r'HostnameData.csv', "write")
